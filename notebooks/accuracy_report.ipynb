{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Accuracy Analysis Report\n",
    "\n",
    "This notebook demonstrates how to load, analyze, and visualize prediction accuracy data from the SenseQuant trading system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The accuracy audit system captures telemetry during backtests and live trading, recording:\n",
    "- Predicted vs. actual trade directions (LONG/SHORT/NOOP)\n",
    "- Entry and exit prices\n",
    "- Holding periods\n",
    "- Realized returns\n",
    "- Feature values and metadata\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load prediction traces from CSV files\n",
    "2. Compute accuracy metrics (precision, recall, F1)\n",
    "3. Analyze financial performance (Sharpe ratio, drawdown, profit factor)\n",
    "4. Generate visualizations (confusion matrix, return distributions)\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Set the `batch_id` parameter to analyze a specific backtest run, or use glob patterns to aggregate multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Modify these parameters\n",
    "batch_id = \"20241012_143000\"  # Replace with your batch ID\n",
    "telemetry_dir = f\"../data/analytics/predictions_{batch_id}_*.csv*\"  # Glob pattern for trace files\n",
    "output_dir = \"../data/reports\"  # Output directory for reports and plots\n",
    "\n",
    "# Create output directory if needed\n",
    "import os\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Batch ID: {batch_id}\")\n",
    "print(f\"Telemetry pattern: {telemetry_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "Import required libraries and modules from the SenseQuant codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# Import SenseQuant modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from services.accuracy_analyzer import AccuracyAnalyzer\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Prediction Traces\n",
    "\n",
    "Load telemetry data from CSV files. The analyzer supports both compressed (.csv.gz) and uncompressed (.csv) formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = AccuracyAnalyzer()\n",
    "\n",
    "# Find all matching telemetry files\n",
    "trace_files = glob.glob(telemetry_dir)\n",
    "\n",
    "if not trace_files:\n",
    "    print(f\"WARNING: No trace files found matching pattern: {telemetry_dir}\")\n",
    "    print(\"Please check the batch_id and telemetry_dir configuration.\")\n",
    "else:\n",
    "    print(f\"Found {len(trace_files)} trace file(s):\")\n",
    "    for f in trace_files:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "# Load all traces\n",
    "all_traces = []\n",
    "for trace_file in trace_files:\n",
    "    try:\n",
    "        traces = analyzer.load_traces(Path(trace_file))\n",
    "        all_traces.extend(traces)\n",
    "        print(f\"Loaded {len(traces)} traces from {trace_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {trace_file}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal traces loaded: {len(all_traces)}\")\n",
    "\n",
    "if len(all_traces) == 0:\n",
    "    print(\"\\nNo traces available for analysis. This notebook requires telemetry data.\")\n",
    "    print(\"Run a backtest with enable_telemetry=True to generate trace files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Trace Data\n",
    "\n",
    "Let's examine the structure and basic statistics of the loaded traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    # Convert traces to DataFrame for easier analysis\n",
    "    traces_df = pd.DataFrame([trace.to_dict() for trace in all_traces])\n",
    "\n",
    "    print(\"=== Trace Data Summary ===\")\n",
    "    print(f\"\\nTotal trades: {len(traces_df)}\")\n",
    "    print(f\"Date range: {traces_df['timestamp'].min()} to {traces_df['timestamp'].max()}\")\n",
    "    print(f\"\\nSymbols: {traces_df['symbol'].unique()}\")\n",
    "    print(f\"Strategies: {traces_df['strategy'].unique()}\")\n",
    "\n",
    "    print(\"\\n=== Direction Distribution ===\")\n",
    "    print(\"\\nPredicted directions:\")\n",
    "    print(traces_df['predicted_direction'].value_counts())\n",
    "    print(\"\\nActual directions:\")\n",
    "    print(traces_df['actual_direction'].value_counts())\n",
    "\n",
    "    print(\"\\n=== Return Statistics ===\")\n",
    "    print(traces_df['realized_return_pct'].describe())\n",
    "\n",
    "    # Display first few traces\n",
    "    print(\"\\n=== Sample Traces (first 5) ===\")\n",
    "    display(traces_df[[\n",
    "        'timestamp', 'symbol', 'strategy',\n",
    "        'predicted_direction', 'actual_direction',\n",
    "        'realized_return_pct'\n",
    "    ]].head())\n",
    "else:\n",
    "    print(\"No traces to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Accuracy Metrics\n",
    "\n",
    "Calculate comprehensive accuracy metrics including:\n",
    "- Classification metrics: precision, recall, F1 score per direction\n",
    "- Confusion matrix showing prediction vs. actual outcomes\n",
    "- Hit ratio (overall accuracy)\n",
    "- Win rate (percentage of profitable trades)\n",
    "- Financial metrics: Sharpe ratio, max drawdown, profit factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    # Compute metrics\n",
    "    metrics = analyzer.compute_metrics(all_traces)\n",
    "\n",
    "    print(\"=== Classification Metrics ===\")\n",
    "    print(f\"\\nHit Ratio (Accuracy): {metrics.hit_ratio:.2%}\")\n",
    "    print(f\"Total Trades: {metrics.total_trades}\")\n",
    "\n",
    "    print(\"\\nPrecision by Direction:\")\n",
    "    for direction, score in metrics.precision.items():\n",
    "        print(f\"  {direction}: {score:.2%}\")\n",
    "\n",
    "    print(\"\\nRecall by Direction:\")\n",
    "    for direction, score in metrics.recall.items():\n",
    "        print(f\"  {direction}: {score:.2%}\")\n",
    "\n",
    "    print(\"\\nF1 Score by Direction:\")\n",
    "    for direction, score in metrics.f1_score.items():\n",
    "        print(f\"  {direction}: {score:.2%}\")\n",
    "\n",
    "    print(\"\\n=== Financial Metrics ===\")\n",
    "    print(f\"Win Rate: {metrics.win_rate:.2%}\")\n",
    "    print(f\"Average Return: {metrics.avg_return:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {metrics.sharpe_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {metrics.max_drawdown:.2f}%\")\n",
    "    print(f\"Profit Factor: {metrics.profit_factor:.2f}\")\n",
    "    print(f\"Average Holding Period: {metrics.avg_holding_minutes:.1f} minutes\")\n",
    "else:\n",
    "    print(\"No traces available for metric computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix Visualization\n",
    "\n",
    "The confusion matrix shows how well predictions match actual outcomes:\n",
    "- Rows: Actual direction (what actually happened)\n",
    "- Columns: Predicted direction (what we predicted)\n",
    "- Diagonal values: Correct predictions\n",
    "- Off-diagonal values: Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    # Create confusion matrix plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    labels = [\"LONG\", \"SHORT\", \"NOOP\"]\n",
    "\n",
    "    sns.heatmap(\n",
    "        metrics.confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        square=True,\n",
    "        linewidths=0.5\n",
    "    )\n",
    "\n",
    "    plt.title('Prediction Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Direction', fontsize=12, labelpad=10)\n",
    "    plt.ylabel('Actual Direction', fontsize=12, labelpad=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_path = Path(output_dir) / f\"confusion_matrix_{batch_id}.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved confusion matrix to: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for confusion matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Return Distribution Analysis\n",
    "\n",
    "Analyze the distribution of realized returns to understand:\n",
    "- Overall profitability\n",
    "- Return symmetry (skewness)\n",
    "- Outliers and tail risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    returns = [t.realized_return_pct for t in all_traces]\n",
    "\n",
    "    # Create return distribution plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(returns, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Break-even')\n",
    "    plt.axvline(x=np.mean(returns), color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {np.mean(returns):.2f}%')\n",
    "    plt.title('Return Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Return (%)', fontsize=11)\n",
    "    plt.ylabel('Frequency', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(returns, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "    plt.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    plt.title('Return Box Plot', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Return (%)', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_path = Path(output_dir) / f\"return_distribution_{batch_id}.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved return distribution to: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\n=== Return Statistics ===\")\n",
    "    print(f\"Mean: {np.mean(returns):.2f}%\")\n",
    "    print(f\"Median: {np.median(returns):.2f}%\")\n",
    "    print(f\"Std Dev: {np.std(returns):.2f}%\")\n",
    "    print(f\"Min: {np.min(returns):.2f}%\")\n",
    "    print(f\"Max: {np.max(returns):.2f}%\")\n",
    "    print(f\"Skewness: {pd.Series(returns).skew():.2f}\")\n",
    "    print(f\"Kurtosis: {pd.Series(returns).kurtosis():.2f}\")\n",
    "else:\n",
    "    print(\"No data available for return distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precision-Recall Analysis\n",
    "\n",
    "Compare precision and recall across different prediction directions to identify strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    # Create precision-recall bar chart\n",
    "    directions = ['LONG', 'SHORT', 'NOOP']\n",
    "    precision_vals = [metrics.precision[d] for d in directions]\n",
    "    recall_vals = [metrics.recall[d] for d in directions]\n",
    "    f1_vals = [metrics.f1_score[d] for d in directions]\n",
    "\n",
    "    x = np.arange(len(directions))\n",
    "    width = 0.25\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.bar(x - width, precision_vals, width, label='Precision', color='skyblue', edgecolor='black')\n",
    "    plt.bar(x, recall_vals, width, label='Recall', color='lightcoral', edgecolor='black')\n",
    "    plt.bar(x + width, f1_vals, width, label='F1 Score', color='lightgreen', edgecolor='black')\n",
    "\n",
    "    plt.xlabel('Direction', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.title('Precision, Recall, and F1 Score by Direction', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x, directions)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_path = Path(output_dir) / f\"precision_recall_{batch_id}.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved precision-recall chart to: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for precision-recall analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cumulative Return Analysis\n",
    "\n",
    "Visualize the cumulative returns over time to understand strategy performance trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    # Sort traces by timestamp\n",
    "    sorted_traces = sorted(all_traces, key=lambda t: t.timestamp)\n",
    "\n",
    "    # Compute cumulative returns\n",
    "    timestamps = [t.timestamp for t in sorted_traces]\n",
    "    returns = [t.realized_return_pct for t in sorted_traces]\n",
    "    cumulative_returns = np.cumsum(returns)\n",
    "\n",
    "    # Plot cumulative returns\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(timestamps, cumulative_returns, linewidth=2, color='steelblue', label='Cumulative Return')\n",
    "    plt.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Break-even')\n",
    "\n",
    "    # Mark drawdown regions\n",
    "    running_max = np.maximum.accumulate(cumulative_returns)\n",
    "    drawdown = cumulative_returns - running_max\n",
    "    plt.fill_between(timestamps, cumulative_returns, running_max,\n",
    "                     where=(drawdown < 0), color='red', alpha=0.2, label='Drawdown')\n",
    "\n",
    "    plt.title('Cumulative Returns Over Time', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=11)\n",
    "    plt.ylabel('Cumulative Return (%)', fontsize=11)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    output_path = Path(output_dir) / f\"cumulative_returns_{batch_id}.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved cumulative returns chart to: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for cumulative return analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Metrics Report\n",
    "\n",
    "Save computed metrics to a JSON file for future reference and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    # Export report to JSON\n",
    "    report_path = Path(output_dir) / f\"accuracy_report_{batch_id}.json\"\n",
    "    analyzer.export_report(metrics, report_path)\n",
    "\n",
    "    print(f\"Metrics report exported to: {report_path}\")\n",
    "\n",
    "    # Display report contents\n",
    "    with open(report_path) as f:\n",
    "        report_data = json.load(f)\n",
    "\n",
    "    print(\"\\n=== Report Contents (preview) ===\")\n",
    "    print(json.dumps(report_data, indent=2, default=str)[:1000] + \"...\")\n",
    "else:\n",
    "    print(\"No data available for report export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations\n",
    "\n",
    "Generate actionable insights based on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_traces) > 0:\n",
    "    print(\"=== Analysis Summary ===\")\n",
    "    print(f\"\\nBatch ID: {batch_id}\")\n",
    "    print(f\"Total Trades Analyzed: {metrics.total_trades}\")\n",
    "    print(f\"Date Range: {traces_df['timestamp'].min()} to {traces_df['timestamp'].max()}\")\n",
    "\n",
    "    print(\"\\n=== Key Performance Indicators ===\")\n",
    "    print(f\"Hit Ratio: {metrics.hit_ratio:.2%}\")\n",
    "    print(f\"Win Rate: {metrics.win_rate:.2%}\")\n",
    "    print(f\"Sharpe Ratio: {metrics.sharpe_ratio:.2f}\")\n",
    "    print(f\"Profit Factor: {metrics.profit_factor:.2f}\")\n",
    "\n",
    "    print(\"\\n=== Recommendations ===\")\n",
    "\n",
    "    # Hit ratio recommendations\n",
    "    if metrics.hit_ratio < 0.5:\n",
    "        print(\"- Low hit ratio (<50%) suggests prediction accuracy needs improvement\")\n",
    "        print(\"  Consider feature engineering or model tuning\")\n",
    "    elif metrics.hit_ratio > 0.7:\n",
    "        print(\"- Strong hit ratio (>70%) indicates good prediction accuracy\")\n",
    "\n",
    "    # Win rate recommendations\n",
    "    if metrics.win_rate < 0.5:\n",
    "        print(\"- Win rate below 50% suggests risk management issues\")\n",
    "        print(\"  Review stop-loss and take-profit settings\")\n",
    "\n",
    "    # Sharpe ratio recommendations\n",
    "    if metrics.sharpe_ratio < 1.0:\n",
    "        print(\"- Sharpe ratio <1.0 indicates poor risk-adjusted returns\")\n",
    "        print(\"  Focus on reducing volatility or improving returns\")\n",
    "    elif metrics.sharpe_ratio > 2.0:\n",
    "        print(\"- Excellent Sharpe ratio (>2.0) shows strong risk-adjusted performance\")\n",
    "\n",
    "    # Direction-specific recommendations\n",
    "    print(\"\\n=== Direction-Specific Insights ===\")\n",
    "    for direction in ['LONG', 'SHORT', 'NOOP']:\n",
    "        if metrics.precision[direction] < 0.4:\n",
    "            print(f\"- {direction} predictions have low precision ({metrics.precision[direction]:.2%})\")\n",
    "            print(f\"  Many {direction} predictions turn out to be incorrect\")\n",
    "        if metrics.recall[direction] < 0.4:\n",
    "            print(f\"- {direction} predictions have low recall ({metrics.recall[direction]:.2%})\")\n",
    "            print(f\"  Missing many actual {direction} opportunities\")\n",
    "\n",
    "    print(\"\\n=== Next Steps ===\")\n",
    "    print(\"1. Compare this batch with historical batches to identify trends\")\n",
    "    print(\"2. Analyze misclassifications in confusion matrix for pattern insights\")\n",
    "    print(\"3. Investigate outlier returns (extreme gains/losses)\")\n",
    "    print(\"4. Consider parameter optimization based on weak areas\")\n",
    "    print(\"5. Run additional backtests with adjusted hyperparameters\")\n",
    "else:\n",
    "    print(\"No data available for summary generation.\")\n",
    "    print(\"\\nTo generate a report:\")\n",
    "    print(\"1. Run a backtest with enable_telemetry=True\")\n",
    "    print(\"2. Update the batch_id parameter in this notebook\")\n",
    "    print(\"3. Re-run all cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Release Audit Summary (US-022)\n\n",
    "This section consolidates telemetry data, optimization results, and student model metrics\n",
    "for release readiness assessment. It provides a unified view of baseline vs optimized performance,\n",
    "student model validation status, and monitoring KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest release audit bundle (if available)\n",
    "import glob\n",
    "from datetime import datetime\n\n",
    "# Find latest audit bundle\n",
    "audit_dirs = sorted(glob.glob(\"../release/audit_*\"), reverse=True)\n\n",
    "if audit_dirs:\n",
    "    latest_audit_dir = Path(audit_dirs[0])\n",
    "    metrics_path = latest_audit_dir / \"metrics.json\"\n",
    "    \n",
    "    if metrics_path.exists():\n",
    "        with open(metrics_path) as f:\n",
    "            audit_metrics = json.load(f)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"RELEASE AUDIT SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nAudit ID: {audit_metrics['audit_id']}\")\n",
    "        print(f\"Audit Date: {audit_metrics['audit_timestamp']}\")\n",
    "        print(f\"Deployment Ready: {'YES' if audit_metrics['deployment_ready'] else 'NO'}\")\n",
    "        \n",
    "        if audit_metrics.get('risk_flags'):\n",
    "            print(f\"\\nRisk Flags ({len(audit_metrics['risk_flags'])}):\")\n",
    "            for flag in audit_metrics['risk_flags']:\n",
    "                print(f\"   - {flag}\")\n",
    "    else:\n",
    "        print(\"Latest audit bundle found but metrics.json is missing\")\n",
    "        audit_metrics = None\n",
    "else:\n",
    "    print(\"No release audit bundles found. Run: python scripts/release_audit.py\")\n",
    "    audit_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline vs Optimized Comparison\n",
    "if audit_metrics and 'baseline' in audit_metrics and 'optimized' in audit_metrics:\n",
    "    baseline = audit_metrics['baseline']\n",
    "    optimized = audit_metrics['optimized']\n",
    "    deltas = audit_metrics.get('deltas', {})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE vs OPTIMIZED CONFIGURATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison_data = [\n",
    "        ['Sharpe Ratio', baseline['sharpe_ratio'], optimized['sharpe_ratio'], \n",
    "         deltas.get('sharpe_ratio_delta', 0.0)],\n",
    "        ['Total Return (%)', baseline['total_return_pct'], optimized['total_return_pct'], \n",
    "         deltas.get('total_return_delta_pct', 0.0)],\n",
    "        ['Win Rate (%)', baseline['win_rate_pct'], optimized['win_rate_pct'], \n",
    "         deltas.get('win_rate_delta_pct', 0.0)],\n",
    "        ['Hit Ratio (%)', baseline['hit_ratio_pct'], optimized['hit_ratio_pct'], \n",
    "         deltas.get('hit_ratio_delta_pct', 0.0)],\n",
    "    ]\n",
    "    \n",
    "    comp_df = pd.DataFrame(comparison_data, \n",
    "                          columns=['Metric', 'Baseline', 'Optimized', 'Delta'])\n",
    "    \n",
    "    print(\"\\n\" + comp_df.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(comparison_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    baseline_vals = [row[1] for row in comparison_data]\n",
    "    optimized_vals = [row[2] for row in comparison_data]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline',\n",
    "                   color='#E74C3C', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, optimized_vals, width, label='Optimized',\n",
    "                   color='#27AE60', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Value', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Release Audit: Baseline vs Optimized Performance',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([row[0] for row in comparison_data], rotation=15, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = Path(output_dir) / \"release_audit_comparison.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nSaved comparison chart to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Baseline/optimized metrics not available in audit bundle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Model Status\n",
    "if audit_metrics and 'student_model' in audit_metrics:\n",
    "    student = audit_metrics['student_model']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STUDENT MODEL VALIDATION STATUS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nDeployed: {'YES' if student.get('deployed') else 'NO'}\")\n",
    "    if student.get('deployed'):\n",
    "        print(f\"Version: {student.get('version', 'unknown')}\")\n",
    "        print(f\"Validation Precision: {student.get('validation_precision', 0.0):.2%}\")\n",
    "        print(f\"Validation Recall: {student.get('validation_recall', 0.0):.2%}\")\n",
    "        print(f\"Test Accuracy: {student.get('test_accuracy', 0.0):.2%}\")\n",
    "        print(f\"Feature Count: {student.get('feature_count', 0)}\")\n",
    "        print(f\"Training Samples: {student.get('training_samples', 0):,}\")\n",
    "else:\n",
    "    print(\"\\nNo student model metrics in audit bundle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring KPIs\n",
    "if audit_metrics and 'monitoring' in audit_metrics:\n",
    "    monitoring = audit_metrics['monitoring']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MONITORING KPIs (Rolling Windows)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'intraday_30day' in monitoring:\n",
    "        intra = monitoring['intraday_30day']\n",
    "        print(\"\\nIntraday Strategy (30-day window):\")\n",
    "        print(f\"   Hit Ratio: {intra.get('hit_ratio', 0.0):.2%}\")\n",
    "        print(f\"   Sharpe Ratio: {intra.get('sharpe_ratio', 0.0):.2f}\")\n",
    "        print(f\"   Alert Count: {intra.get('alert_count', 0)}\")\n",
    "        print(f\"   Degradation: {'YES' if intra.get('degradation_detected') else 'NO'}\")\n",
    "    \n",
    "    if 'swing_90day' in monitoring:\n",
    "        swing = monitoring['swing_90day']\n",
    "        print(\"\\nSwing Strategy (90-day window):\")\n",
    "        print(f\"   Precision (LONG): {swing.get('precision_long', 0.0):.2%}\")\n",
    "        print(f\"   Recall (LONG): {swing.get('recall_long', 0.0):.2%}\")\n",
    "        print(f\"   Max Drawdown: {swing.get('max_drawdown_pct', 0.0):.1f}%\")\n",
    "        print(f\"   Alert Count: {swing.get('alert_count', 0)}\")\n",
    "        print(f\"   Degradation: {'YES' if swing.get('degradation_detected') else 'NO'}\")\n",
    "else:\n",
    "    print(\"\\nNo monitoring metrics in audit bundle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Audit Recommendations\n\n",
    "Based on the consolidated audit metrics:\n\n",
    "1. **If Deployment Ready**: Proceed with gradual rollout as per deployment plan\n",
    "2. **If Risk Flags Present**: Address each flag before production deployment\n",
    "3. **Monitor KPIs**: Continue tracking rolling window metrics for early degradation detection\n",
    "4. **Schedule Next Audit**: Plan monthly audits to maintain release readiness\n\n",
    "For full audit details, review:\n",
    "- `release/audit_<timestamp>/summary.md` - Executive summary\n",
    "- `release/audit_<timestamp>/metrics.json` - Complete metrics\n",
    "- `release/audit_<timestamp>/plots/` - All visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive analysis of prediction accuracy and financial performance. Use these insights to:\n",
    "\n",
    "- Identify model weaknesses and areas for improvement\n",
    "- Compare performance across different time periods or parameter settings\n",
    "- Make data-driven decisions about strategy modifications\n",
    "- Track improvements over time as you iterate on the trading system\n",
    "\n",
    "For questions or issues, refer to the SenseQuant documentation or accuracy analyzer source code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}