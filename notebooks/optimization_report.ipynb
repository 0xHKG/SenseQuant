{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Optimization Analysis Report (US-019)\n",
    "\n",
    "This notebook analyzes parameter optimization results from batch optimization runs, providing:\n",
    "- Baseline vs optimized configuration comparison\n",
    "- Before/after performance metrics visualization\n",
    "- Parameter sensitivity analysis\n",
    "- Top configuration rankings\n",
    "- Deployment recommendations\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Run an optimization: `python scripts/optimize.py --config ... --run-baseline --export-report`\n",
    "2. Note the output directory (e.g., `data/optimization/run_20241012_143022`)\n",
    "3. Update `optimization_run_dir` below\n",
    "4. Run all cells to generate the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "optimization_run_dir = \"../data/optimization/run_20241012_143022\"  # Replace with your run directory\n",
    "output_dir = \"../data/reports\"  # Output directory for generated plots\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Optimization run: {optimization_run_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✅ Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Optimization Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_dir = Path(optimization_run_dir)\n",
    "\n",
    "# Load baseline metrics\n",
    "baseline_path = opt_dir / \"baseline_metrics.json\"\n",
    "if baseline_path.exists():\n",
    "    with open(baseline_path) as f:\n",
    "        baseline_metrics = json.load(f)\n",
    "    print(f\"✅ Loaded baseline metrics: Sharpe={baseline_metrics.get('sharpe_ratio', 0.0):.2f}\")\n",
    "else:\n",
    "    baseline_metrics = {}\n",
    "    print(\"⚠️  No baseline metrics found\")\n",
    "\n",
    "# Load configurations\n",
    "configs_path = opt_dir / \"configs.json\"\n",
    "if configs_path.exists():\n",
    "    with open(configs_path) as f:\n",
    "        configs = json.load(f)\n",
    "    print(f\"✅ Loaded {len(configs)} configurations\")\n",
    "else:\n",
    "    configs = []\n",
    "    print(\"⚠️  No configurations found\")\n",
    "\n",
    "# Load ranked results CSV\n",
    "ranked_path = opt_dir / \"ranked_results.csv\"\n",
    "if ranked_path.exists():\n",
    "    ranked_df = pd.read_csv(ranked_path)\n",
    "    print(f\"✅ Loaded ranked results: {len(ranked_df)} rows\")\n",
    "else:\n",
    "    ranked_df = pd.DataFrame()\n",
    "    print(\"⚠️  No ranked results found\")\n",
    "\n",
    "# Load optimization summary\n",
    "summary_path = opt_dir / \"optimization_summary.json\"\n",
    "if summary_path.exists():\n",
    "    with open(summary_path) as f:\n",
    "        opt_summary = json.load(f)\n",
    "    print(\"\\n✅ Optimization Summary:\")\n",
    "    print(f\"   Strategy: {opt_summary.get('strategy', 'unknown')}\")\n",
    "    print(f\"   Symbols: {opt_summary.get('symbols', [])}\")\n",
    "    print(f\"   Date range: {opt_summary.get('start_date')} to {opt_summary.get('end_date')}\")\n",
    "    print(f\"   Total configs: {opt_summary.get('total_configs', 0)}\")\n",
    "    print(f\"   Successful: {opt_summary.get('successful_configs', 0)}\")\n",
    "else:\n",
    "    opt_summary = {}\n",
    "    print(\"⚠️  No optimization summary found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline vs Best Configuration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs:\n",
    "    best_config = configs[0]\n",
    "\n",
    "    if best_config.get('metrics'):\n",
    "        best_metrics = best_config['metrics']\n",
    "        best_acc = best_config.get('accuracy_metrics', {})\n",
    "\n",
    "        print(\"=\"*70)\n",
    "        print(\"BASELINE vs BEST CONFIGURATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n🏆 Best Config: {best_config['config_id']}\")\n",
    "        print(f\"   Rank: #{best_config['rank']}\")\n",
    "        print(f\"   Composite Score: {best_config['score']:.3f}\")\n",
    "\n",
    "        print(\"\\n📈 Financial Metrics Comparison:\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        metrics_comparison = [\n",
    "            (\"Sharpe Ratio\",\n",
    "             baseline_metrics.get('sharpe_ratio', 0.0),\n",
    "             best_metrics.get('sharpe_ratio', 0.0), \"\"),\n",
    "            (\"Total Return\",\n",
    "             baseline_metrics.get('total_return', 0.0) * 100,\n",
    "             best_metrics.get('total_return_pct', 0.0), \"%\"),\n",
    "            (\"Win Rate\",\n",
    "             baseline_metrics.get('win_rate', 0.0) * 100,\n",
    "             best_metrics.get('win_rate_pct', 0.0), \"%\"),\n",
    "        ]\n",
    "\n",
    "        for metric_name, baseline_val, best_val, unit in metrics_comparison:\n",
    "            delta = best_val - baseline_val\n",
    "            arrow = \"↑\" if delta > 0 else \"↓\" if delta < 0 else \"→\"\n",
    "            color = \"green\" if delta > 0 else \"red\" if delta < 0 else \"yellow\"\n",
    "            print(f\"{metric_name:20s}: {baseline_val:8.2f}{unit:1s} → {best_val:8.2f}{unit:1s}  ({arrow} {delta:+.2f}{unit:1s})\")\n",
    "\n",
    "        if best_acc and baseline_metrics.get('precision_long') is not None:\n",
    "            print(\"\\n🎯 Accuracy Metrics Comparison:\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "            acc_comparison = [\n",
    "                (\"Precision (LONG)\",\n",
    "                 baseline_metrics.get('precision_long', 0.0) * 100,\n",
    "                 best_acc.get('precision_long', 0.0) * 100),\n",
    "                (\"Hit Ratio\",\n",
    "                 baseline_metrics.get('hit_ratio', 0.0) * 100,\n",
    "                 best_acc.get('hit_ratio', 0.0) * 100),\n",
    "                (\"Recall (LONG)\",\n",
    "                 baseline_metrics.get('recall_long', 0.0) * 100,\n",
    "                 best_acc.get('recall_long', 0.0) * 100),\n",
    "            ]\n",
    "\n",
    "            for metric_name, baseline_val, best_val in acc_comparison:\n",
    "                delta = best_val - baseline_val\n",
    "                arrow = \"↑\" if delta > 0 else \"↓\" if delta < 0 else \"→\"\n",
    "                print(f\"{metric_name:20s}: {baseline_val:7.1f}% → {best_val:7.1f}%  ({arrow} {delta:+.1f}pp)\")\n",
    "\n",
    "        print(\"\\n⚙️  Best Configuration Parameters:\")\n",
    "        print(\"-\" * 70)\n",
    "        for param, value in best_config.get('parameters', {}).items():\n",
    "            print(f\"   {param:30s}: {value}\")\n",
    "        print(\"=\"*70)\n",
    "else:\n",
    "    print(\"⚠️  No configurations available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Before/After Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs and baseline_metrics and configs[0].get('metrics'):\n",
    "    best_config = configs[0]\n",
    "\n",
    "    # Prepare data\n",
    "    categories = ['Sharpe\\nRatio', 'Total Return\\n(%)', 'Win Rate\\n(%)',\n",
    "                 'Precision\\n(LONG %)', 'Hit Ratio\\n(%)']\n",
    "\n",
    "    baseline_vals = [\n",
    "        baseline_metrics.get('sharpe_ratio', 0.0),\n",
    "        baseline_metrics.get('total_return', 0.0) * 100,\n",
    "        baseline_metrics.get('win_rate', 0.0) * 100,\n",
    "        baseline_metrics.get('precision_long', 0.0) * 100,\n",
    "        baseline_metrics.get('hit_ratio', 0.0) * 100,\n",
    "    ]\n",
    "\n",
    "    best_vals = [\n",
    "        best_config['metrics'].get('sharpe_ratio', 0.0),\n",
    "        best_config['metrics'].get('total_return_pct', 0.0),\n",
    "        best_config['metrics'].get('win_rate_pct', 0.0),\n",
    "        best_config.get('accuracy_metrics', {}).get('precision_long', 0.0) * 100,\n",
    "        best_config.get('accuracy_metrics', {}).get('hit_ratio', 0.0) * 100,\n",
    "    ]\n",
    "\n",
    "    # Create comparison chart\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline',\n",
    "                   color='#E74C3C', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "    bars2 = ax.bar(x + width/2, best_vals, width, label='Best Configuration',\n",
    "                   color='#27AE60', alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                   f'{height:.1f}', ha='center', va='bottom',\n",
    "                   fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('Metrics', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel('Value', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    ax.set_title('Before/After Optimization: Performance Comparison',\n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories, fontsize=11)\n",
    "    ax.legend(fontsize=12, loc='upper left', frameon=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = Path(output_dir) / \"before_after_comparison.png\"\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Saved to: {output_path}\")\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️  Insufficient data for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ranked_df.empty and 'score' in ranked_df.columns:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PARAMETER SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nAnalyzing parameter impact on composite score...\\n\")\n",
    "\n",
    "    # Identify parameter columns\n",
    "    exclude_cols = ['config_id', 'rank', 'score', 'sharpe_ratio', 'total_return',\n",
    "                   'precision_long', 'recall_long', 'hit_ratio', 'win_rate']\n",
    "    param_cols = [col for col in ranked_df.columns if col not in exclude_cols]\n",
    "\n",
    "    if param_cols:\n",
    "        # Calculate correlations\n",
    "        correlations = {}\n",
    "        for param in param_cols:\n",
    "            if pd.api.types.is_numeric_dtype(ranked_df[param]):\n",
    "                corr = ranked_df[param].corr(ranked_df['score'])\n",
    "                if not np.isnan(corr):\n",
    "                    correlations[param] = corr\n",
    "\n",
    "        if correlations:\n",
    "            # Sort by absolute correlation\n",
    "            sorted_corr = sorted(correlations.items(),\n",
    "                               key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "            print(\"Top Parameters by Impact (Correlation with Score):\\n\")\n",
    "            for param, corr in sorted_corr[:8]:\n",
    "                impact = \"Positive (↑ increases score)\" if corr > 0 else \"Negative (↓ decreases score)\"\n",
    "                strength = \"Strong\" if abs(corr) > 0.5 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
    "                print(f\"   {param:30s}: {corr:+.3f}  ({strength}, {impact})\")\n",
    "\n",
    "            # Visualization\n",
    "            fig, ax = plt.subplots(figsize=(14, max(6, len(sorted_corr) * 0.4)))\n",
    "\n",
    "            params = [item[0] for item in sorted_corr]\n",
    "            corr_values = [item[1] for item in sorted_corr]\n",
    "            colors = ['#27AE60' if c > 0 else '#E74C3C' for c in corr_values]\n",
    "\n",
    "            bars = ax.barh(params, corr_values, color=colors, alpha=0.7,\n",
    "                          edgecolor='black', linewidth=1.2)\n",
    "\n",
    "            ax.axvline(x=0, color='black', linestyle='-', linewidth=1.5)\n",
    "            ax.set_xlabel('Correlation with Composite Score',\n",
    "                         fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Parameter', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Parameter Sensitivity Analysis',\n",
    "                        fontsize=14, fontweight='bold', pad=15)\n",
    "            ax.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "\n",
    "            # Value labels\n",
    "            for bar, value in zip(bars, corr_values, strict=False):\n",
    "                x_pos = value + (0.02 if value > 0 else -0.02)\n",
    "                ha = 'left' if value > 0 else 'right'\n",
    "                ax.text(x_pos, bar.get_y() + bar.get_height()/2,\n",
    "                       f'{value:.3f}', ha=ha, va='center',\n",
    "                       fontsize=9, fontweight='bold')\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            output_path = Path(output_dir) / \"parameter_sensitivity.png\"\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\n✅ Saved to: {output_path}\")\n",
    "\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"⚠️  No numeric parameters for correlation\")\n",
    "    else:\n",
    "        print(\"⚠️  No parameters found\")\n",
    "else:\n",
    "    print(\"⚠️  No ranked data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top 5 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs:\n",
    "    print(\"=\"*70)\n",
    "    print(\"TOP 5 CONFIGURATIONS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    top_5 = configs[:5]\n",
    "\n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for cfg in top_5:\n",
    "        if cfg.get('metrics'):\n",
    "            row = {\n",
    "                'Rank': cfg['rank'],\n",
    "                'Config ID': cfg['config_id'],\n",
    "                'Score': f\"{cfg['score']:.3f}\",\n",
    "                'Sharpe': f\"{cfg['metrics'].get('sharpe_ratio', 0.0):.2f}\",\n",
    "                'Return (%)': f\"{cfg['metrics'].get('total_return_pct', 0.0):.1f}\",\n",
    "            }\n",
    "\n",
    "            if cfg.get('accuracy_metrics'):\n",
    "                row['Precision (%)'] = f\"{cfg['accuracy_metrics'].get('precision_long', 0.0) * 100:.1f}\"\n",
    "                row['Hit Ratio (%)'] = f\"{cfg['accuracy_metrics'].get('hit_ratio', 0.0) * 100:.1f}\"\n",
    "\n",
    "            comparison_data.append(row)\n",
    "\n",
    "    if comparison_data:\n",
    "        top_df = pd.DataFrame(comparison_data)\n",
    "        print(top_df.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "        # Visualization\n",
    "        scores = [float(cfg['score']) for cfg in top_5 if cfg.get('score')]\n",
    "\n",
    "        if scores:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            x = np.arange(len(scores))\n",
    "            colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(scores)))\n",
    "\n",
    "            bars = ax.bar(x, scores, color=colors, edgecolor='black',\n",
    "                         linewidth=1.2, alpha=0.8)\n",
    "\n",
    "            # Value labels\n",
    "            for bar, score in zip(bars, scores, strict=False):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                       bar.get_height() + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom',\n",
    "                       fontsize=10, fontweight='bold')\n",
    "\n",
    "            ax.set_xlabel('Configuration Rank', fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel('Composite Score', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Top 5 Configurations by Composite Score',\n",
    "                        fontsize=14, fontweight='bold', pad=15)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([f\"#{i+1}\" for i in range(len(scores))],\n",
    "                              fontsize=11)\n",
    "            ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            output_path = Path(output_dir) / \"top_5_configurations.png\"\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\n✅ Saved to: {output_path}\")\n",
    "\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"⚠️  No configurations available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs:\n",
    "    best_config = configs[0]\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nBased on optimization results, recommended deployment plan:\\n\")\n",
    "\n",
    "    print(\"📋 Phase 1: Validation (2 weeks)\")\n",
    "    print(\"   - Deploy best configuration in paper trading mode\")\n",
    "    print(\"   - Monitor live accuracy metrics via dashboard\")\n",
    "    print(\"   - Compare vs baseline in real-time\")\n",
    "    print(\"   - Alert on precision < 65% or Sharpe < 1.3\\n\")\n",
    "\n",
    "    print(\"📋 Phase 2: Gradual Rollout (4 weeks)\")\n",
    "    print(\"   - Week 1-2: 20% capital allocation\")\n",
    "    print(\"   - Week 3: 50% capital allocation\")\n",
    "    print(\"   - Week 4: 100% capital if validation successful\\n\")\n",
    "\n",
    "    print(\"📋 Phase 3: Full Production\")\n",
    "    print(\"   - Update config.py with validated parameters\")\n",
    "    print(\"   - Archive baseline configuration\")\n",
    "    print(\"   - Update dashboard alert thresholds\\n\")\n",
    "\n",
    "    print(\"⚠️  Rollback Plan:\")\n",
    "    print(\"   If live metrics degrade:\")\n",
    "    print(\"   1. Revert to baseline config immediately\")\n",
    "    print(\"   2. Analyze live telemetry for root cause\")\n",
    "    print(\"   3. Re-run optimization on recent data\")\n",
    "    print(\"   4. Consider adaptive parameter adjustment\\n\")\n",
    "\n",
    "    print(\"🔧 Recommended Parameters:\")\n",
    "    print(f\"   Config ID: {best_config['config_id']}\")\n",
    "    for param, value in best_config.get('parameters', {}).items():\n",
    "        print(f\"   {param:30s}: {value}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs:\n",
    "    summary_report = {\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"optimization_run\": optimization_run_dir,\n",
    "        \"baseline_metrics\": baseline_metrics,\n",
    "        \"best_config\": configs[0] if configs else None,\n",
    "        \"total_configs\": len(configs),\n",
    "        \"strategy\": opt_summary.get('strategy'),\n",
    "        \"symbols\": opt_summary.get('symbols'),\n",
    "        \"date_range\": {\n",
    "            \"start\": opt_summary.get('start_date'),\n",
    "            \"end\": opt_summary.get('end_date')\n",
    "        }\n",
    "    }\n",
    "\n",
    "    report_path = Path(output_dir) / \"optimization_analysis_summary.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"✅ Exported analysis summary to: {report_path}\")\n",
    "    print(\"\\n📊 Analysis complete! Review generated visualizations and reports.\")\n",
    "else:\n",
    "    print(\"⚠️  No data to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
